""" 
Name            StudentID

Man Shuen Pun    N10377328
Haoqing Pan      N10315497
Tin-Gin Yeh    N7097085

"""


import numpy as np

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import regularizers
from tensorflow.keras import layers
from sklearn.utils import shuffle
import tensorflow_datasets as tfds 
from keras import backend as K

from sklearn import model_selection
import matplotlib.pyplot as plt
import random


class Dataset:
    # This class will facilitate the creation of a few-shot dataset
    # from the Omniglot dataset that can be sampled from quickly while also
    # allowing to create new labels at the same time.
    def __init__(self, split):
        # Download the tfrecord files containing the omniglot data and convert to a dataset.
        dataset, ds_info = tfds.load("omniglot", split=split, as_supervised=True, shuffle_files=False,with_info=True)
        self.dataset=dataset
        # Iterate over the dataset to get each individual image and its class,
        # and put that data into a dictionary.
        self.data = {}
        # print(ds_info)

        def extraction(image, label):
            # This function will shrink the Omniglot images to the desired size,
            # scale pixel values and convert the RGB image to grayscale
            image = tf.image.convert_image_dtype(image, tf.float32)# (105,105,3) - 3: RGB
            image = tf.image.rgb_to_grayscale(image)
            image = tf.image.resize(image, [105, 105])# (105,105,1) 1: grey scale
            return image, label

        for image, label in dataset.map(extraction):
            # print(label)
            image = image.numpy()
            label = str(label.numpy())
            if label not in self.data:
                self.data[label] = []
            self.data[label].append(image)
            self.labels = list(self.data.keys())
        # self.input_sharp=self.data[self.labels[0]].shape[1:]
        self.num_classes = len(np.unique(self.labels))

def merge_dataset(pairs1, pairLabels1, pairs2, pairLabels2):
    """
    Merge training data and testing data 
    
    Arguments:
        pairs1 
        pairLabels1 
        pairs2 
        pairLabels2 
    
    Returns:
        pairs -- pairs1+pairs2
        pairLabels -- pairLabels1+pairLabels2
    """
    pairs = np.concatenate((pairs1, pairs2))  # combine training images and testing images
    pairLabels = np.concatenate((pairLabels1, pairLabels2))  # combine training labels and testing labels

    return pairs, pairLabels

def contrastive_loss(y_true, y_pred):
    """
    Contrastive loss function
    
    Arguments:
        y_true  -- y_true=0 for positive pair, y_true=1 for negative pair
        y_pred  -- Euclidean distance between f(Pi) and f(Pj)
    
    """
    margin = 1  # margin>0 determines how far the embeddings of a negative pair should be pushed apart
    square_pred = K.square(y_pred) # D(Pi,Pj)(2), according to formula
    margin_square = K.square(K.maximum(margin - y_pred,0)) # according to formula
    return K.mean(0.5 * (1-y_true) * square_pred + 0.5 * y_true * margin_square) # according to formula

def test_contrastive_loss():
    """
    A loss function written with Numpy to compare with loss function written with Tensorflow.
    To validate the correctness of the contrastive loss function written with Tensorflow.
    """

    num_data = 100 #number of data generated
    margin = 1 # margin>0 determines how far the embeddings of a negative pair should be pushed apart

    y_pred_data=np.random.rand(num_data).astype(np.float32) #simulation data for the prediction generated by the Siamese network
    y_true_data=np.random.randint(2,size=(num_data)) #simulation data for the true labels of each pair, 0 or 1

    #Compute loss with numpy
    loss_np=0.
    for i in range(num_data):
        
        if(y_true_data[i]==0): 
            square_pred = np.square(y_pred_data[i]) # D(Pi,Pj)(2), according to formula
            loss_np += 0.5 * square_pred
        elif(y_true_data[i]==1):
            margin_square = 0.5 * np.square(max(margin-y_pred_data[i],0)) #according to formula
            loss_np += margin_square
        
    loss_np /= num_data #get mean of loss
    print('Contrastive loss computed with numpy', loss_np)

def test_triplet_loss():
    num_data = 10
    feat_dim = 6
    margin = 0.2
    
    embeddings = [np.random.rand(num_data, feat_dim).astype(np.float32),
                  np.random.rand(num_data, feat_dim).astype(np.float32),
                  np.random.rand(num_data, feat_dim).astype(np.float32)]
    labels = np.random.randint(0, 1, size=(num_data)).astype(np.float32)
    #Compute loss with numpy
    loss_np = 0.
    anchor = embeddings[0]
    positive = embeddings[1]
    negative = embeddings[2]
    for i in range(num_data):
        pos_dist = np.sum(np.square(anchor[i] - positive[i]))
        neg_dist = np.sum(np.square(anchor[i] - negative[i]))
        loss_np += max(pos_dist-neg_dist+margin, 0.)
    loss_np /= num_data
    print('Triplet loss computed with numpy', loss_np)

def euclidean_distance(vects):
    """
    Calculate Euclidean distance between two vectors
    
    Arguments:
        vects  -- two vectors
    
    """
    x, y = vects
    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True) #according to Euclidean formula
    return K.sqrt(K.maximum(sum_square, K.epsilon()))

def triplet_loss(y_true, y_pred, margin = 0.4):
    """
    Implementation of the triplet loss function
    Arguments:
    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.
    y_pred -- python list containing three objects:
            anchor -- the encodings for the anchor data
            positive -- the encodings for the positive data (similar to anchor)
            negative -- the encodings for the negative data (different from anchor)
    Returns:
    loss -- real number, value of the loss
    """

    anchor = y_pred[0]
    positive = y_pred[1]
    negative = y_pred[2]

    # distance between the anchor and the positive
    pos_dist = tf.reduce_sum(tf.square(anchor-positive),axis=1)

    # distance between the anchor and the negative
    neg_dist = tf.reduce_sum(tf.square(anchor-negative),axis=1)

    # compute loss
    basic_loss = pos_dist-neg_dist+margin
    loss = tf.maximum(basic_loss,0.0)
    loss = tf.reduce_mean(loss)     
    return loss

def accuracy(y_true, y_pred):
    """
    Accuracy metrics used during the training of the Siamese network
    
    Arguments:
        y_true  -- real labels, 0-positive pair, 1-negative pair
        y_pred  -- predicted values computed by the Siamese network
    
    Returns:
        tensor -- A bool tensor with the mean of elements of which are true positive and true negative 
    """
    tensor_positive=K.equal((1-y_true), K.cast(y_pred < 0.5, y_true.dtype)) # a bool tensor that represent true positive, e.g.(0,1,1,0,0,0,0,1...)
    tensor_negative=K.equal(y_true, K.cast(y_pred >= 0.5, y_true.dtype)) # a bool tensor that represent true negative,    e.g.(0,0,0,1,1,1,1,0...)
    
    bool_tensor_acc=tf.math.logical_or(tensor_positive,tensor_negative) # 'or' operation of two bool sensors, integrate all true positive and true negative

    return K.mean(bool_tensor_acc)

def Load_omniglot():
    '''
    load 2 datasets for train and test respectively

    Returns:
        train_dataset -- dataset instance for train
        test_dataset -- dataset instance for test
    '''
    train_dataset = Dataset("train")
    test_dataset = Dataset("test")
    
    return train_dataset,test_dataset

def create_pairs(dataset):

    """
    split pairs of positive pair and negitive pair

    Arguments:
        dataset: dataset instance for making pairs
    """

    pairs=[] # store positive pairs as well as negative pairs
    pairLabels=[] # 0-positive, 1-negative

    numOfLabels=len(dataset.labels)

    for i in range(numOfLabels):
        for j in range(len(dataset.data[dataset.labels[i]])-1):
            
            pairs.append([
                dataset.data[dataset.labels[i]][j],
                dataset.data[dataset.labels[i]][j+1]
            ]) # a positive pair of images
            pairLabels.append(0) # label for positve pair is 0

            negative_lable=i
            while negative_lable == i:
                negative_lable=random.randint(1,numOfLabels-1) # only when negative_lable is not current lable then it stop
            randomNum=random.randint(1,len(dataset.data[dataset.labels[negative_lable]])-1)
            
            pairs.append([
                dataset.data[dataset.labels[i]][j],
                dataset.data[dataset.labels[negative_lable]][randomNum]
            ]) # a negative pair of images
            pairLabels.append(1) # label for negative pair is 1


    pairs=np.asarray(pairs) # convert to numpy array
    pairLabels=np.asarray(pairLabels)

    return pairs,pairLabels

def verificate_dataset(pairs,pairLabels,number):
    """
    Plot the pairs to visually validate if pairs are created properly

    Arguments:
        pairs  -- list of pairs of both positive pairs and negative pairs
        pairLabels  -- the labels for each pair
        number -- number of pairs needed to plot
    """

    fig=plt.figure(figsize=(10, 10)) # create a figure of size 10*10
    
    columns = 2 #number of images in a row, a pair has 2 images
    rows = number #number of pairs needed to plot
    for i in range(1, columns*rows+1):
        fig.add_subplot(rows, columns, i)
        
        pixels=pairs[(i-1)//2][(i-1)%2] #take each image out
        pixels=pixels*255 #since we have rescale from (0,255)-->(0,1), we need to rescale it back
        pixels = np.array(pixels, dtype='uint8')
        plt.title('Label is {label}'.format(label=pairLabels[(i-1)//2])) #show the label as title
        plt.axis('off')
        plt.imshow(pixels,cmap='gray')
        
    plt.show()

def create_cnn_network(input_shape):
    """
    Create a general CNN network which can deal with any shape of input
    
    Arguments:
        input_shape 
    
    Returns:
        Model -- groups layers into an object with training and inference features
    """
    def conv_bn(x):
        x = layers.Conv2D(filters=64, kernel_size=3, strides=2, padding="same")(x)
        x = layers.BatchNormalization()(x)
        return layers.ReLU()(x)

    inputs = layers.Input(shape=(105, 105, 1))
    x = conv_bn(inputs)
    x = conv_bn(x)
    x = conv_bn(x)
    x = conv_bn(x)
    x = layers.Flatten()(x)
    outputs = layers.Dense(5, activation="softmax")(x)
    return keras.Model(inputs=inputs, outputs=outputs)

def create_siamese_network(cnn_network,input_shape):
    """
    Create Siamese network based on the base CNN network and the input shape
    
    Arguments:
        cnn_network {keras.models.Model} -- CNN base network
        input_shape  -- input shape
    
    Returns:
        Model -- Siamese model
    """
    input_a=keras.layers.Input(shape=input_shape)
    input_b=keras.layers.Input(shape=input_shape)

    # re-use the same instance `cnn_network`, 
    # the weights of the network will be shared across the two branches
    processed_a=cnn_network(input_a)
    processed_b=cnn_network(input_b)

    distance=keras.layers.Lambda(euclidean_distance)([processed_a,processed_b]) #use of Euclidean formula to calculate the distance between two vectors
    # prediction = Dense(units=1, activation='sigmoid')(distance)
    # model = Model(
    #     inputs=[input_a, input_b], outputs=prediction)
    model=keras.models.Model([input_a,input_b],distance)

    return model

def train_siamese_network(siamese_network,train_pairs,val_pairs,train_lables,val_labels):
    """
    Train siamese network
    
    Arguments:
        siamese_network {keras.models.Model} -- defined Siamese model
        train_pairs  -- training pairs of images
        val_pairs  -- validation pairs of images
        train_lables  -- training labels
        val_labels  -- validation labels
    
    Returns:
        Model -- a trained Siamese model
        history -- log of the training history, used for plotting accuracy and loss
    """
    
    optimizer = keras.optimizers.RMSprop()
    siamese_network.compile(loss=contrastive_loss,optimizer=optimizer, metrics=[accuracy]) # specify loss function, optimizer and metrics to calculate accuracy
    history=siamese_network.fit([train_pairs[:,0],train_pairs[:,1]],train_lables, 
                        batch_size=128, #batch size
                        epochs=epochs, #can be modified, 30 epochs for now
                        validation_data=([val_pairs[:,0],val_pairs[:,1]],val_labels)) #validation pairs and labels
    
    return siamese_network,history

def plot_training_accuracy_and_loss(history):
    """
    Plot the accuracy and loss during the training
    
    Arguments:
        history  -- log of the training history
    """

    # Plot training & validation accuracy values
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('Model accuracy') #title
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train','Validation'], loc='upper left')
    plt.show()

    # Plot training & validation accuracy values
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model loss') #title
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train','Validation'], loc='upper left')
    plt.show()

def evaluate_generalization_capability(pairs,pairLabels):
    """
    Compute final accuracy on the specified datasets, and display the results with added description
    
    Arguments:
        pairs  -- pairs of images
        pairLabels  -- labels for the pairs
        desc_datasets  -- desciption of the datasets
    """

    y_pred=trained_siamese_model.predict([pairs[:,0],pairs[:,1]]) #get the prediction by feeding the dataset into the trained model
    acc=compute_accuracy(pairLabels,y_pred) #get the accuracy by comparing the predictions with real labels
    print('* Accuracy: %0.2f%%' % (100 * acc))

def compute_accuracy(y_true, y_pred):
    
    """
    Compute classification accuracy with a fixed threshold on distances.
    
    Arguments:
        y_true  -- real labels, 0-positive pair, 1-negative pair
        y_pred  -- predicted values computed by the Siamese network
    
    Returns:
        acc -- accuracy of the model
    """
   
    n = y_true.shape[0] #the number of total pairs

    true_positive_prediction=(1-y_true[y_pred.ravel() < 0.5]).sum() # the number of true positive

    true_negative_prediction=y_true[y_pred.ravel() >= 0.5].sum() # the number of true negative

    acc = (true_positive_prediction + true_negative_prediction) / n  # calculate accuracy by (True Positive + True Negative)/n
    return acc



epochs=15 #number of epochs

if __name__ == '__main__':

    test_contrastive_loss() #test the contrastive loss function
    test_triplet_loss() #test the triplet loss function

    # load datasets
    train_dataset,test_dataset=Load_omniglot()

    # split pairs for training
    pairs,pairLabels=create_pairs(train_dataset)
    input_shape=(105, 105, 1)

    # verify dataset and pairs
    verificate_dataset(pairs,pairLabels,12) # visually verificate if the pairs created properly

    # 80% for trian and 20% for test
    train_pairs, val_pairs, train_labels, val_labels = model_selection.train_test_split(pairs,pairLabels,test_size=0.2) 

    cnn_network = create_cnn_network(input_shape) # get a default CNN network
    siamese_network = create_siamese_network(cnn_network,input_shape) #get a Siamese model based on the base CNN model and the input shape

    # train the siamese network with training pairs,labels and validation pairs,labels
    trained_siamese_model, history = train_siamese_network(siamese_network,train_pairs,val_pairs,train_labels,val_labels) 
    plot_training_accuracy_and_loss(history) #use the 'history' to plot the training & validation accuracy and loss

    # testing it with pairs from the set of images with labels for training
    evaluate_generalization_capability(pairs,pairLabels)
    # testing it with pairs from the set of images with labels only for testing
    test_pairs, test_labels=create_pairs(test_dataset)
    evaluate_generalization_capability(test_pairs,test_labels)
    # testing it with pairs from the set of images with all labels,
    all_pairs,all_labels=merge_dataset(pairs,pairLabels,test_pairs,test_labels)
    evaluate_generalization_capability(all_pairs,all_labels)
    '''
    pass
    '''